{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99be7354",
   "metadata": {},
   "source": [
    "# Phase 4. Extensions\n",
    "\n",
    "## Pipeline requested for the extension\n",
    "\n",
    "**Object detection**\n",
    "\n",
    "- leveraging already existing and trained YOLO model we'll be able to find BBox for RGB images\n",
    "\n",
    "**ROI**\n",
    "\n",
    "- for each BBox crop RGB image accordingly\n",
    "- crop the same exact point on corresponding depth file\n",
    "\n",
    "**Feat extraction**\n",
    "\n",
    "Feature extraction happens leveraging two different CNNs:\n",
    "- *RGB Branch*: feature extraction from RGB cropped image\n",
    "- *Depth Branch*: feature extraction from depth cropped image. Depth is threated as a 2D image, not a cloud of dots.\n",
    "\n",
    "**Fusion**\n",
    "\n",
    "$f_{\\text{fused}}=concat(f_{\\text{rgb}},f_{\\text{depth}})$\n",
    "\n",
    "**Pose estimation**\n",
    "\n",
    "Pose estimation is done by a regressor (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b5d74",
   "metadata": {},
   "source": [
    "# Step 1: Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801cf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from config import Config\n",
    "from models.yolo_detector import YOLODetector\n",
    "\n",
    "print(f\"‚úÖ Imports completati\")\n",
    "print(f\"   Device: {Config.DEVICE}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained YOLO model\n",
    "yolo_ckpt = Config.CHECKPOINT_DIR / 'yolo' / 'yolo_train20' / 'weights' / 'best.pt'\n",
    "\n",
    "if yolo_ckpt.exists():\n",
    "    yolo_detector = YOLODetector(\n",
    "        model_name=str(yolo_ckpt),\n",
    "        num_classes=Config.NUM_CLASSES\n",
    "    )\n",
    "    print(f\"‚úÖ YOLO loaded from: {yolo_ckpt}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"YOLO checkpoint not found: {yolo_ckpt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df29d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test YOLO detection on a sample image\n",
    "from dataset.linemod_pose import LineMODPoseDataset\n",
    "\n",
    "# Load test dataset to get sample images\n",
    "test_dataset = LineMODPoseDataset(\n",
    "    dataset_root=Config.LINEMOD_ROOT,\n",
    "    split='test',\n",
    "    crop_margin=Config.POSE_CROP_MARGIN,\n",
    "    output_size=Config.POSE_IMAGE_SIZE\n",
    ")\n",
    "\n",
    "# Get a sample\n",
    "sample = test_dataset[0]\n",
    "rgb_path = sample['rgb_path']\n",
    "depth_path = sample['depth_path']\n",
    "gt_bbox = sample['bbox'].numpy()  # Ground truth bbox [x, y, w, h]\n",
    "\n",
    "print(f\"üì∑ Sample image: {rgb_path}\")\n",
    "print(f\"üìè GT BBox [x,y,w,h]: {gt_bbox}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad311f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run YOLO detection\n",
    "image_bgr = cv2.imread(rgb_path)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "detections = yolo_detector.detect_objects(image_bgr, conf_threshold=0.3)\n",
    "\n",
    "print(f\"üéØ Detected {len(detections)} object(s)\")\n",
    "for i, det in enumerate(detections):\n",
    "    print(f\"   [{i+1}] Class: {det['class_name']}, Conf: {det['confidence']:.2f}, BBox: {det['bbox']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detection results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Original image with GT bbox\n",
    "axes[0].imshow(image_rgb)\n",
    "x, y, w, h = gt_bbox\n",
    "rect_gt = plt.Rectangle((x, y), w, h, fill=False, edgecolor='green', linewidth=2, label='GT')\n",
    "axes[0].add_patch(rect_gt)\n",
    "axes[0].set_title('Ground Truth BBox')\n",
    "axes[0].legend()\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Image with YOLO detections\n",
    "axes[1].imshow(image_rgb)\n",
    "for det in detections:\n",
    "    x1, y1, x2, y2 = det['bbox']\n",
    "    rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='red', linewidth=2)\n",
    "    axes[1].add_patch(rect)\n",
    "    axes[1].text(x1, y1-5, f\"{det['class_name']} {det['confidence']:.2f}\", \n",
    "                 color='red', fontsize=10, backgroundcolor='white')\n",
    "axes[1].set_title(f'YOLO Detections ({len(detections)} objects)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Step 1 completed: Object detection with pre-trained YOLO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6f15d",
   "metadata": {},
   "source": [
    "# Step 2: ROI - Crop RGB and Depth\n",
    "\n",
    "For each detected bounding box:\n",
    "1. Crop the RGB image with a margin\n",
    "2. Crop the corresponding depth map at the **same exact coordinates**\n",
    "3. Resize both to a fixed size (224x224) for the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ca971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi(image: np.ndarray, bbox_xyxy: np.ndarray, margin: float = 0.15, output_size: int = 224):\n",
    "    \"\"\"\n",
    "    Crop a region of interest from an image given a bounding box.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (H, W, C) for RGB or (H, W) for depth\n",
    "        bbox_xyxy: Bounding box [x1, y1, x2, y2]\n",
    "        margin: Margin to add around the bbox (as fraction of bbox size)\n",
    "        output_size: Output size for the crop (square)\n",
    "    \n",
    "    Returns:\n",
    "        Cropped and resized image\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox_xyxy\n",
    "    w, h = x2 - x1, y2 - y1\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    \n",
    "    # Add margin and make it square\n",
    "    size = max(w, h) * (1 + margin)\n",
    "    half = size / 2\n",
    "    \n",
    "    # Compute crop coordinates (clipped to image bounds)\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    x1_crop = int(max(0, cx - half))\n",
    "    y1_crop = int(max(0, cy - half))\n",
    "    x2_crop = int(min(img_w, cx + half))\n",
    "    y2_crop = int(min(img_h, cy + half))\n",
    "    \n",
    "    # Crop\n",
    "    if image.ndim == 3:\n",
    "        crop = image[y1_crop:y2_crop, x1_crop:x2_crop, :]\n",
    "    else:\n",
    "        crop = image[y1_crop:y2_crop, x1_crop:x2_crop]\n",
    "    \n",
    "    # Resize to output size\n",
    "    crop_resized = cv2.resize(crop, (output_size, output_size), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return crop_resized\n",
    "\n",
    "\n",
    "# Test on the first detection\n",
    "if len(detections) > 0:\n",
    "    det = detections[0]\n",
    "    bbox = det['bbox']\n",
    "    \n",
    "    print(f\"üéØ Using detection: {det['class_name']} (conf: {det['confidence']:.2f})\")\n",
    "    print(f\"   BBox [x1,y1,x2,y2]: {bbox}\")\n",
    "else:\n",
    "    # Fallback to GT bbox if no detection\n",
    "    x, y, w, h = gt_bbox\n",
    "    bbox = np.array([x, y, x+w, y+h])\n",
    "    print(f\"‚ö†Ô∏è No YOLO detection, using GT bbox: {bbox}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d4b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop RGB image\n",
    "rgb_crop = crop_roi(image_rgb, bbox, margin=0.15, output_size=224)\n",
    "\n",
    "# Load and crop depth image\n",
    "depth_raw = np.array(Image.open(depth_path))  # uint16, values in mm\n",
    "depth_crop = crop_roi(depth_raw, bbox, margin=0.15, output_size=224)\n",
    "\n",
    "# Normalize depth to [0, 1] for visualization and network input\n",
    "DEPTH_MAX = 2000.0  # mm (typical max depth in LineMOD)\n",
    "depth_crop_normalized = np.clip(depth_crop / DEPTH_MAX, 0, 1)\n",
    "\n",
    "print(f\"‚úÖ ROI crops created:\")\n",
    "print(f\"   RGB crop shape: {rgb_crop.shape}\")\n",
    "print(f\"   Depth crop shape: {depth_crop.shape}\")\n",
    "print(f\"   Depth range: [{depth_crop.min():.0f}, {depth_crop.max():.0f}] mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the crops\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# RGB crop\n",
    "axes[0].imshow(rgb_crop)\n",
    "axes[0].set_title(f'RGB Crop ({rgb_crop.shape[0]}x{rgb_crop.shape[1]})')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Depth crop (normalized for visualization)\n",
    "im = axes[1].imshow(depth_crop_normalized, cmap='viridis')\n",
    "axes[1].set_title(f'Depth Crop (normalized)')\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im, ax=axes[1], fraction=0.046, label='Depth (normalized)')\n",
    "\n",
    "# RGB + Depth overlay\n",
    "axes[2].imshow(rgb_crop)\n",
    "axes[2].imshow(depth_crop_normalized, cmap='viridis', alpha=0.5)\n",
    "axes[2].set_title('RGB + Depth Overlay')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Step 2 completed: ROI crops for RGB and Depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c56c2d",
   "metadata": {},
   "source": [
    "# Step 3: Feature Extraction\n",
    "\n",
    "- **RGB Branch**: ResNet-50 backbone ‚Üí 2048-dim features\n",
    "- **Depth Branch**: DepthEncoder CNN ‚Üí 256-dim features (see `models/depth_encoder.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction models\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import torch.nn as nn\n",
    "from models.depth_encoder import DepthEncoder\n",
    "\n",
    "# RGB Branch: ResNet-50 (pretrained, without final FC)\n",
    "resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "rgb_encoder = nn.Sequential(*list(resnet.children())[:-1]).to(Config.DEVICE)\n",
    "rgb_encoder.eval()\n",
    "\n",
    "# Depth Branch: DepthEncoder\n",
    "depth_encoder = DepthEncoder(output_dim=256).to(Config.DEVICE)\n",
    "depth_encoder.eval()\n",
    "\n",
    "print(f\"‚úÖ Feature extractors loaded:\")\n",
    "print(f\"   RGB: ResNet-50 ‚Üí 2048-dim\")\n",
    "print(f\"   Depth: DepthEncoder ‚Üí 256-dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tensors\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# ImageNet normalization for RGB\n",
    "imagenet_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# RGB tensor\n",
    "rgb_tensor = torch.from_numpy(rgb_crop).permute(2, 0, 1).float() / 255.0\n",
    "rgb_tensor = imagenet_normalize(rgb_tensor).unsqueeze(0).to(Config.DEVICE)\n",
    "\n",
    "# Depth tensor\n",
    "depth_tensor = torch.from_numpy(depth_crop_normalized).float().unsqueeze(0).unsqueeze(0).to(Config.DEVICE)\n",
    "\n",
    "print(f\"‚úÖ Input tensors: RGB {rgb_tensor.shape}, Depth {depth_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    f_rgb = rgb_encoder(rgb_tensor).squeeze()      # (2048,)\n",
    "    f_depth = depth_encoder(depth_tensor).squeeze() # (256,)\n",
    "\n",
    "print(f\"‚úÖ Step 3 completed:\")\n",
    "print(f\"   f_rgb: {f_rgb.shape}\")\n",
    "print(f\"   f_depth: {f_depth.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd772ed",
   "metadata": {},
   "source": [
    "# Step 4: Fusion\n",
    "\n",
    "Late fusion via concatenation:\n",
    "\n",
    "$$f_{\\text{fused}} = \\text{concat}(f_{\\text{rgb}}, f_{\\text{depth}}) \\in \\mathbb{R}^{2304}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ec192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Late fusion: concatenate RGB and Depth features\n",
    "f_fused = torch.cat([f_rgb, f_depth], dim=0)\n",
    "\n",
    "print(f\"‚úÖ Step 4 completed: Feature Fusion\")\n",
    "print(f\"   f_rgb:   {f_rgb.shape[0]} dims\")\n",
    "print(f\"   f_depth: {f_depth.shape[0]} dims\")\n",
    "print(f\"   f_fused: {f_fused.shape[0]} dims (concatenated)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fused feature vector\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Fused features as heatmap (reshaped for visualization)\n",
    "f_np = f_fused.cpu().numpy()\n",
    "# Reshape to 2D for better visualization (48x48 = 2304)\n",
    "f_2d = f_np.reshape(48, 48)\n",
    "im = axes[0].imshow(f_2d, cmap='coolwarm', aspect='auto')\n",
    "axes[0].set_title(f'Fused Features (2304 dims reshaped to 48√ó48)')\n",
    "axes[0].set_xlabel('Feature index')\n",
    "axes[0].set_ylabel('Feature index')\n",
    "plt.colorbar(im, ax=axes[0], fraction=0.046)\n",
    "\n",
    "# Show contribution of each branch\n",
    "axes[1].barh(['Depth\\n(256 dims)', 'RGB\\n(2048 dims)'], [256, 2048], color=['orange', 'steelblue'])\n",
    "axes[1].set_xlabel('Feature dimensions')\n",
    "axes[1].set_title('Feature Contribution per Branch')\n",
    "for i, v in enumerate([256, 2048]):\n",
    "    axes[1].text(v + 50, i, f'{v} ({v/2304*100:.1f}%)', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c23009",
   "metadata": {},
   "source": [
    "# Step 5: Pose Estimation\n",
    "\n",
    "MLP regressor predicts 6D pose from fused features:\n",
    "- **Quaternion** (4D): $[q_w, q_x, q_y, q_z]$ normalized to unit norm\n",
    "- **Translation** (3D): $[t_x, t_y, t_z]$ in meters\n",
    "\n",
    "See `models/pose_regressor.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9936db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose regressor\n",
    "from models.pose_regressor import PoseRegressor\n",
    "\n",
    "pose_regressor = PoseRegressor(input_dim=2304, dropout=0.3).to(Config.DEVICE)\n",
    "pose_regressor.eval()\n",
    "\n",
    "# Predict pose (model has random weights - just testing the pipeline)\n",
    "with torch.no_grad():\n",
    "    f_fused_batch = f_fused.unsqueeze(0)  # (1, 2304)\n",
    "    pose_pred = pose_regressor(f_fused_batch).squeeze()  # (7,)\n",
    "\n",
    "quat_pred = pose_pred[:4].cpu().numpy()\n",
    "trans_pred = pose_pred[4:].cpu().numpy()\n",
    "\n",
    "print(f\"‚úÖ Step 5 completed: Pose Estimation\")\n",
    "print(f\"   Quaternion [qw,qx,qy,qz]: {quat_pred}\")\n",
    "print(f\"   Quaternion norm: {np.linalg.norm(quat_pred):.6f}\")\n",
    "print(f\"   Translation [tx,ty,tz]: {trans_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with ground truth\n",
    "gt_quat = sample['quaternion'].numpy()\n",
    "gt_trans = sample['translation'].numpy()\n",
    "\n",
    "print(f\"üìä Comparison (random weights vs GT):\")\n",
    "print(f\"   Predicted quat: {quat_pred}\")\n",
    "print(f\"   GT quat:        {gt_quat}\")\n",
    "print(f\"   Predicted trans: {trans_pred}\")\n",
    "print(f\"   GT trans:        {gt_trans}\")\n",
    "\n",
    "print(f\"\\nüéâ Pipeline complete! Next: train the model on LineMOD dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d830830",
   "metadata": {},
   "source": [
    "# Step 6: Training\n",
    "\n",
    "Train the complete RGB-D fusion model end-to-end:\n",
    "- **RGB Encoder**: ResNet-50 (pretrained, fine-tuned)\n",
    "- **Depth Encoder**: DepthEncoder (trained from scratch)\n",
    "- **Pose Regressor**: MLP (trained from scratch)\n",
    "\n",
    "Loss: Geodesic (rotation) + Smooth L1 (translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAIN_CONFIG = {\n",
    "    'epochs': 50,\n",
    "    'batch_size': 16,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lambda_rot': 1,\n",
    "    'lambda_trans': 1,\n",
    "}\n",
    "\n",
    "print(f\"üìã Training configuration:\")\n",
    "for k, v in TRAIN_CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for RGB-D fusion training\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class LineMODFusionDataset(Dataset):\n",
    "    \"\"\"Dataset that returns RGB crop, Depth crop, and pose labels.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dataset, crop_margin=0.15, output_size=224, depth_max=2000.0):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.crop_margin = crop_margin\n",
    "        self.output_size = output_size\n",
    "        self.depth_max = depth_max\n",
    "        self.imagenet_normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.base_dataset[idx]\n",
    "        \n",
    "        # Load full images\n",
    "        rgb_full = cv2.cvtColor(cv2.imread(sample['rgb_path']), cv2.COLOR_BGR2RGB)\n",
    "        depth_full = np.array(Image.open(sample['depth_path']))\n",
    "        \n",
    "        # Get bbox (convert from [x,y,w,h] to [x1,y1,x2,y2])\n",
    "        bbox = sample['bbox'].numpy()\n",
    "        x, y, w, h = bbox\n",
    "        bbox_xyxy = np.array([x, y, x+w, y+h])\n",
    "        \n",
    "        # Crop both RGB and Depth at same coordinates\n",
    "        rgb_crop = crop_roi(rgb_full, bbox_xyxy, self.crop_margin, self.output_size)\n",
    "        depth_crop = crop_roi(depth_full, bbox_xyxy, self.crop_margin, self.output_size)\n",
    "        \n",
    "        # Normalize depth to [0, 1]\n",
    "        depth_crop = np.clip(depth_crop / self.depth_max, 0, 1).astype(np.float32)\n",
    "        \n",
    "        # Convert RGB to tensor and normalize\n",
    "        rgb_tensor = torch.from_numpy(rgb_crop).permute(2, 0, 1).float() / 255.0\n",
    "        rgb_tensor = self.imagenet_normalize(rgb_tensor)\n",
    "        \n",
    "        # Convert depth to tensor\n",
    "        depth_tensor = torch.from_numpy(depth_crop).unsqueeze(0).float()\n",
    "        \n",
    "        return {\n",
    "            'rgb': rgb_tensor,\n",
    "            'depth': depth_tensor,\n",
    "            'quaternion': sample['quaternion'],\n",
    "            'translation': sample['translation']\n",
    "        }\n",
    "\n",
    "# Create dataloaders\n",
    "full_train = LineMODPoseDataset(Config.LINEMOD_ROOT, split='train')\n",
    "full_test = LineMODPoseDataset(Config.LINEMOD_ROOT, split='test')\n",
    "\n",
    "# Train/val split\n",
    "train_len = int(len(full_train) * 0.85)\n",
    "val_len = len(full_train) - train_len\n",
    "train_base, val_base = random_split(full_train, [train_len, val_len], \n",
    "                                     generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataset = LineMODFusionDataset(train_base)\n",
    "val_dataset = LineMODFusionDataset(val_base)\n",
    "test_dataset_fusion = LineMODFusionDataset(full_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_CONFIG['batch_size'], shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=TRAIN_CONFIG['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"‚úÖ Datasets created:\")\n",
    "print(f\"   Train: {len(train_dataset)} samples\")\n",
    "print(f\"   Val: {len(val_dataset)} samples\")\n",
    "print(f\"   Test: {len(test_dataset_fusion)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ddb2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fresh models for training\n",
    "#rgb_encoder_train = nn.Sequential(*list(models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).children())[:-1])\n",
    "rgb_encoder_train = nn.Sequential(*list(models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).children())[:-1])\n",
    "depth_encoder_train = DepthEncoder(output_dim=256)\n",
    "pose_regressor_train = PoseRegressor(input_dim=2304, dropout=0.3)\n",
    "\n",
    "# Move to device\n",
    "rgb_encoder_train = rgb_encoder_train.to(Config.DEVICE)\n",
    "depth_encoder_train = depth_encoder_train.to(Config.DEVICE)\n",
    "pose_regressor_train = pose_regressor_train.to(Config.DEVICE)\n",
    "\n",
    "# Optimizer (all parameters)\n",
    "all_params = list(rgb_encoder_train.parameters()) + \\\n",
    "             list(depth_encoder_train.parameters()) + \\\n",
    "             list(pose_regressor_train.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(all_params, lr=TRAIN_CONFIG['lr'], weight_decay=TRAIN_CONFIG['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_CONFIG['epochs'])\n",
    "\n",
    "# Loss functions\n",
    "from utils.losses import PoseLoss\n",
    "criterion = PoseLoss(lambda_trans=TRAIN_CONFIG['lambda_trans'], lambda_rot=TRAIN_CONFIG['lambda_rot'])\n",
    "\n",
    "print(f\"‚úÖ Models and optimizer initialized\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in all_params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ae66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_epoch(rgb_enc, depth_enc, pose_reg, loader, optimizer, criterion, device):\n",
    "    rgb_enc.train()\n",
    "    depth_enc.train()\n",
    "    pose_reg.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        rgb = batch['rgb'].to(device)\n",
    "        depth = batch['depth'].to(device)\n",
    "        gt_quat = batch['quaternion'].to(device)\n",
    "        gt_trans = batch['translation'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        f_rgb = rgb_enc(rgb).squeeze(-1).squeeze(-1)\n",
    "        f_depth = depth_enc(depth)\n",
    "        f_fused = torch.cat([f_rgb, f_depth], dim=1)\n",
    "        pose = pose_reg(f_fused)\n",
    "        \n",
    "        pred_quat = pose[:, :4]\n",
    "        pred_trans = pose[:, 4:]\n",
    "        \n",
    "        # Loss (PoseLoss returns a dict with 'total', 'rot', 'trans')\n",
    "        loss_dict = criterion(pred_quat, pred_trans, gt_quat, gt_trans)\n",
    "        loss = loss_dict['total']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate_epoch(rgb_enc, depth_enc, pose_reg, loader, criterion, device):\n",
    "    rgb_enc.eval()\n",
    "    depth_enc.eval()\n",
    "    pose_reg.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            rgb = batch['rgb'].to(device)\n",
    "            depth = batch['depth'].to(device)\n",
    "            gt_quat = batch['quaternion'].to(device)\n",
    "            gt_trans = batch['translation'].to(device)\n",
    "            \n",
    "            f_rgb = rgb_enc(rgb).squeeze(-1).squeeze(-1)\n",
    "            f_depth = depth_enc(depth)\n",
    "            f_fused = torch.cat([f_rgb, f_depth], dim=1)\n",
    "            pose = pose_reg(f_fused)\n",
    "            \n",
    "            pred_quat = pose[:, :4]\n",
    "            pred_trans = pose[:, 4:]\n",
    "            \n",
    "            loss_dict = criterion(pred_quat, pred_trans, gt_quat, gt_trans)\n",
    "            loss = loss_dict['total']\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "checkpoint_dir = Config.CHECKPOINT_DIR / 'pose' / 'fusion_rgbd'\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üöÄ Starting training for {TRAIN_CONFIG['epochs']} epochs...\")\n",
    "print(f\"   Checkpoint dir: {checkpoint_dir}\")\n",
    "\n",
    "for epoch in range(TRAIN_CONFIG['epochs']):\n",
    "    train_loss = train_epoch(\n",
    "        rgb_encoder_train, depth_encoder_train, pose_regressor_train,\n",
    "        train_loader, optimizer, criterion, Config.DEVICE\n",
    "    )\n",
    "    val_loss = validate_epoch(\n",
    "        rgb_encoder_train, depth_encoder_train, pose_regressor_train,\n",
    "        val_loader, criterion, Config.DEVICE\n",
    "    )\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'rgb_encoder': rgb_encoder_train.state_dict(),\n",
    "            'depth_encoder': depth_encoder_train.state_dict(),\n",
    "            'pose_regressor': pose_regressor_train.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'val_loss': val_loss\n",
    "        }, checkpoint_dir / 'best.pt')\n",
    "        print(f\"Epoch {epoch+1:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} ‚≠ê (best)\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0207c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(train_losses, label='Train Loss', color='steelblue')\n",
    "ax.plot(val_losses, label='Val Loss', color='orange')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('RGB-D Fusion Training')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Final losses:\")\n",
    "print(f\"   Train: {train_losses[-1]:.4f}\")\n",
    "print(f\"   Val: {val_losses[-1]:.4f}\")\n",
    "print(f\"   Best Val: {best_val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
