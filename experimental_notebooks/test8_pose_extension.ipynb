{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99be7354",
   "metadata": {},
   "source": [
    "# Phase 4. Extensions\n",
    "\n",
    "## Pipeline requested for the extension\n",
    "\n",
    "**Object detection**\n",
    "\n",
    "- leveraging already existing and trained YOLO model we'll be able to find BBox for RGB images\n",
    "\n",
    "**ROI**\n",
    "\n",
    "- for each BBox crop RGB image accordingly\n",
    "- crop the same exact point on corresponding depth file\n",
    "\n",
    "**Feat extraction**\n",
    "\n",
    "Feature extraction happens leveraging two different CNNs:\n",
    "- *RGB Branch*: feature extraction from RGB cropped image\n",
    "- *Depth Branch*: feature extraction from depth cropped image. Depth is threated as a 2D image, not a cloud of dots.\n",
    "\n",
    "**Fusion**\n",
    "\n",
    "$f_{\\text{fused}}=concat(f_{\\text{rgb}},f_{\\text{depth}})$\n",
    "\n",
    "**Pose estimation**\n",
    "\n",
    "Pose estimation is done by a regressor (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b5d74",
   "metadata": {},
   "source": [
    "# Step 1: Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801cf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from config import Config\n",
    "from models.yolo_detector import YOLODetector\n",
    "\n",
    "print(f\"âœ… Imports completati\")\n",
    "print(f\"   Device: {Config.DEVICE}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained YOLO model\n",
    "yolo_ckpt = Config.CHECKPOINT_DIR / 'yolo' / 'yolo_train20' / 'weights' / 'best.pt'\n",
    "\n",
    "if yolo_ckpt.exists():\n",
    "    yolo_detector = YOLODetector(\n",
    "        model_name=str(yolo_ckpt),\n",
    "        num_classes=Config.NUM_CLASSES\n",
    "    )\n",
    "    print(f\"âœ… YOLO loaded from: {yolo_ckpt}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"YOLO checkpoint not found: {yolo_ckpt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df29d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test YOLO detection on a sample image\n",
    "from dataset.linemod_pose import LineMODPoseDataset\n",
    "\n",
    "# Load test dataset to get sample images\n",
    "test_dataset = LineMODPoseDataset(\n",
    "    dataset_root=Config.LINEMOD_ROOT,\n",
    "    split='test',\n",
    "    crop_margin=Config.POSE_CROP_MARGIN,\n",
    "    output_size=Config.POSE_IMAGE_SIZE\n",
    ")\n",
    "\n",
    "# Get a sample\n",
    "sample = test_dataset[0]\n",
    "rgb_path = sample['rgb_path']\n",
    "depth_path = sample['depth_path']\n",
    "gt_bbox = sample['bbox'].numpy()  # Ground truth bbox [x, y, w, h]\n",
    "\n",
    "print(f\"ðŸ“· Sample image: {rgb_path}\")\n",
    "print(f\"ðŸ“ GT BBox [x,y,w,h]: {gt_bbox}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad311f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run YOLO detection\n",
    "image_bgr = cv2.imread(rgb_path)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "detections = yolo_detector.detect_objects(image_bgr, conf_threshold=0.3)\n",
    "\n",
    "print(f\"ðŸŽ¯ Detected {len(detections)} object(s)\")\n",
    "for i, det in enumerate(detections):\n",
    "    print(f\"   [{i+1}] Class: {det['class_name']}, Conf: {det['confidence']:.2f}, BBox: {det['bbox']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detection results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Original image with GT bbox\n",
    "axes[0].imshow(image_rgb)\n",
    "x, y, w, h = gt_bbox\n",
    "rect_gt = plt.Rectangle((x, y), w, h, fill=False, edgecolor='green', linewidth=2, label='GT')\n",
    "axes[0].add_patch(rect_gt)\n",
    "axes[0].set_title('Ground Truth BBox')\n",
    "axes[0].legend()\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Image with YOLO detections\n",
    "axes[1].imshow(image_rgb)\n",
    "for det in detections:\n",
    "    x1, y1, x2, y2 = det['bbox']\n",
    "    rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='red', linewidth=2)\n",
    "    axes[1].add_patch(rect)\n",
    "    axes[1].text(x1, y1-5, f\"{det['class_name']} {det['confidence']:.2f}\", \n",
    "                 color='red', fontsize=10, backgroundcolor='white')\n",
    "axes[1].set_title(f'YOLO Detections ({len(detections)} objects)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Step 1 completed: Object detection with pre-trained YOLO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6f15d",
   "metadata": {},
   "source": [
    "# Step 2: ROI - Crop RGB and Depth\n",
    "\n",
    "For each detected bounding box:\n",
    "1. Crop the RGB image with a margin\n",
    "2. Crop the corresponding depth map at the **same exact coordinates**\n",
    "3. Resize both to a fixed size (224x224) for the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ca971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi(image: np.ndarray, bbox_xyxy: np.ndarray, margin: float = 0.15, output_size: int = 224):\n",
    "    \"\"\"\n",
    "    Crop a region of interest from an image given a bounding box.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (H, W, C) for RGB or (H, W) for depth\n",
    "        bbox_xyxy: Bounding box [x1, y1, x2, y2]\n",
    "        margin: Margin to add around the bbox (as fraction of bbox size)\n",
    "        output_size: Output size for the crop (square)\n",
    "    \n",
    "    Returns:\n",
    "        Cropped and resized image\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox_xyxy\n",
    "    w, h = x2 - x1, y2 - y1\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    \n",
    "    # Add margin and make it square\n",
    "    size = max(w, h) * (1 + margin)\n",
    "    half = size / 2\n",
    "    \n",
    "    # Compute crop coordinates (clipped to image bounds)\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    x1_crop = int(max(0, cx - half))\n",
    "    y1_crop = int(max(0, cy - half))\n",
    "    x2_crop = int(min(img_w, cx + half))\n",
    "    y2_crop = int(min(img_h, cy + half))\n",
    "    \n",
    "    # Crop\n",
    "    if image.ndim == 3:\n",
    "        crop = image[y1_crop:y2_crop, x1_crop:x2_crop, :]\n",
    "    else:\n",
    "        crop = image[y1_crop:y2_crop, x1_crop:x2_crop]\n",
    "    \n",
    "    # Resize to output size\n",
    "    interp = cv2.INTER_LINEAR if image.ndim == 3 else cv2.INTER_NEAREST\n",
    "    crop_resized = cv2.resize(crop, (output_size, output_size), interpolation=interp)\n",
    "    \n",
    "    return crop_resized\n",
    "\n",
    "\n",
    "# Test on the first detection\n",
    "if len(detections) > 0:\n",
    "    det = detections[0]\n",
    "    bbox = det['bbox']\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Using detection: {det['class_name']} (conf: {det['confidence']:.2f})\")\n",
    "    print(f\"   BBox [x1,y1,x2,y2]: {bbox}\")\n",
    "else:\n",
    "    # Fallback to GT bbox if no detection\n",
    "    x, y, w, h = gt_bbox\n",
    "    bbox = np.array([x, y, x+w, y+h])\n",
    "    print(f\"âš ï¸ No YOLO detection, using GT bbox: {bbox}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d4b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop RGB image\n",
    "rgb_crop = crop_roi(image_rgb, bbox, margin=0.15, output_size=224)\n",
    "\n",
    "# Load and crop depth image\n",
    "depth_raw = np.array(Image.open(depth_path))  # uint16, values in mm\n",
    "depth_crop = crop_roi(depth_raw, bbox, margin=0.15, output_size=224)\n",
    "\n",
    "# Normalize depth to [0, 1] for visualization and network input\n",
    "DEPTH_MAX = 2000.0  # mm (typical max depth in LineMOD)\n",
    "depth_crop_normalized = np.clip(depth_crop / DEPTH_MAX, 0, 1)\n",
    "\n",
    "print(f\"âœ… ROI crops created:\")\n",
    "print(f\"   RGB crop shape: {rgb_crop.shape}\")\n",
    "print(f\"   Depth crop shape: {depth_crop.shape}\")\n",
    "print(f\"   Depth range: [{depth_crop.min():.0f}, {depth_crop.max():.0f}] mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the crops\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# RGB crop\n",
    "axes[0].imshow(rgb_crop)\n",
    "axes[0].set_title(f'RGB Crop ({rgb_crop.shape[0]}x{rgb_crop.shape[1]})')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Depth crop (normalized for visualization)\n",
    "im = axes[1].imshow(depth_crop_normalized, cmap='viridis')\n",
    "axes[1].set_title(f'Depth Crop (normalized)')\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im, ax=axes[1], fraction=0.046, label='Depth (normalized)')\n",
    "\n",
    "# RGB + Depth overlay\n",
    "axes[2].imshow(rgb_crop)\n",
    "axes[2].imshow(depth_crop_normalized, cmap='viridis', alpha=0.5)\n",
    "axes[2].set_title('RGB + Depth Overlay')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Step 2 completed: ROI crops for RGB and Depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c56c2d",
   "metadata": {},
   "source": [
    "# Step 3: Feature Extraction\n",
    "\n",
    "- **RGB Branch**: ResNet-18 backbone â†’ 512-dim features\n",
    "- **Depth Branch**: DepthEncoder CNN â†’ 512-dim features (see `models/depth_encoder.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction models\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import torch.nn as nn\n",
    "from models.depth_encoder import DepthEncoder\n",
    "\n",
    "# RGB Branch: ResNet-18 (pretrained, without final FC)\n",
    "resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "rgb_encoder = nn.Sequential(*list(resnet.children())[:-1]).to(Config.DEVICE)\n",
    "rgb_encoder.eval()\n",
    "\n",
    "# Depth Branch: DepthEncoder\n",
    "depth_encoder = DepthEncoder(output_dim=512).to(Config.DEVICE)\n",
    "depth_encoder.eval()\n",
    "\n",
    "print(f\"âœ… Feature extractors loaded:\")\n",
    "print(f\"   RGB: ResNet-18 â†’ 512-dim\")\n",
    "print(f\"   Depth: DepthEncoder â†’ 512-dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tensors\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# ImageNet normalization for RGB\n",
    "imagenet_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# RGB tensor\n",
    "rgb_tensor = torch.from_numpy(rgb_crop).permute(2, 0, 1).float() / 255.0\n",
    "rgb_tensor = imagenet_normalize(rgb_tensor).unsqueeze(0).to(Config.DEVICE)\n",
    "\n",
    "# Depth tensor\n",
    "depth_tensor = torch.from_numpy(depth_crop_normalized).float().unsqueeze(0).unsqueeze(0).to(Config.DEVICE)\n",
    "\n",
    "print(f\"âœ… Input tensors: RGB {rgb_tensor.shape}, Depth {depth_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    f_rgb = rgb_encoder(rgb_tensor).squeeze()      # (512,)\n",
    "    f_depth = depth_encoder(depth_tensor).squeeze() # (512,)\n",
    "\n",
    "print(f\"âœ… Step 3 completed:\")\n",
    "print(f\"   f_rgb: {f_rgb.shape}\")\n",
    "print(f\"   f_depth: {f_depth.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd772ed",
   "metadata": {},
   "source": [
    "# Step 4: Fusion\n",
    "\n",
    "Late fusion via concatenation:\n",
    "\n",
    "$$f_{\\text{fused}} = \\text{concat}(f_{\\text{rgb}}, f_{\\text{depth}}) \\in \\mathbb{R}^{1024}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ec192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Late fusion: concatenate RGB and Depth features\n",
    "f_fused = torch.cat([f_rgb, f_depth], dim=0)\n",
    "\n",
    "print(f\"âœ… Step 4 completed: Feature Fusion\")\n",
    "print(f\"   f_rgb:   {f_rgb.shape[0]} dims\")\n",
    "print(f\"   f_depth: {f_depth.shape[0]} dims\")\n",
    "print(f\"   f_fused: {f_fused.shape[0]} dims (concatenated)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fused feature vector\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Fused features as heatmap (reshaped for visualization)\n",
    "f_np = f_fused.cpu().numpy()\n",
    "# Reshape to 2D for better visualization (32x32 = 1024)\n",
    "f_2d = f_np.reshape(32, 32)\n",
    "im = axes[0].imshow(f_2d, cmap='coolwarm', aspect='auto')\n",
    "axes[0].set_title(f'Fused Features (1024 dims reshaped to 32Ã—32)')\n",
    "axes[0].set_xlabel('Feature index')\n",
    "axes[0].set_ylabel('Feature index')\n",
    "plt.colorbar(im, ax=axes[0], fraction=0.046)\n",
    "\n",
    "# Show contribution of each branch\n",
    "axes[1].barh(['Depth\\n(512 dims)', 'RGB\\n(512 dims)'], [512, 512], color=['orange', 'steelblue'])\n",
    "axes[1].set_xlabel('Feature dimensions')\n",
    "axes[1].set_title('Feature Contribution per Branch')\n",
    "for i, v in enumerate([512, 512]):\n",
    "    axes[1].text(v + 20, i, f'{v} ({v/1024*100:.1f}%)', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c23009",
   "metadata": {},
   "source": [
    "# Step 5: Pose Estimation\n",
    "\n",
    "MLP regressor predicts 6D pose from fused features:\n",
    "- **Quaternion** (4D): $[q_w, q_x, q_y, q_z]$ normalized to unit norm\n",
    "- **Translation** (3D): $[t_x, t_y, t_z]$ in meters\n",
    "\n",
    "Input: 1024-dim fused features (512 RGB + 512 Depth)\n",
    "\n",
    "See `models/pose_regressor.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9936db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose regressor (for demo - without meta features)\n",
    "from models.pose_regressor import PoseRegressor\n",
    "\n",
    "# Note: This is just for pipeline demonstration\n",
    "# During training, we'll use meta features â†’ input_dim=1056\n",
    "pose_regressor = PoseRegressor(input_dim=1024, dropout=0.3).to(Config.DEVICE)\n",
    "pose_regressor.eval()\n",
    "\n",
    "# Predict pose (model has random weights - just testing the pipeline)\n",
    "with torch.no_grad():\n",
    "    f_fused_batch = f_fused.unsqueeze(0)  # (1, 1024)\n",
    "    pose_pred = pose_regressor(f_fused_batch).squeeze()  # (7,)\n",
    "\n",
    "quat_pred = pose_pred[:4].cpu().numpy()\n",
    "trans_pred = pose_pred[4:].cpu().numpy()\n",
    "\n",
    "print(f\"âœ… Step 5 completed: Pose Estimation\")\n",
    "print(f\"   Quaternion [qw,qx,qy,qz]: {quat_pred}\")\n",
    "print(f\"   Quaternion norm: {np.linalg.norm(quat_pred):.6f}\")\n",
    "print(f\"   Translation [tx,ty,tz]: {trans_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with ground truth\n",
    "gt_quat = sample['quaternion'].numpy()\n",
    "gt_trans = sample['translation'].numpy()\n",
    "\n",
    "print(f\"ðŸ“Š Comparison (random weights vs GT):\")\n",
    "print(f\"   Predicted quat: {quat_pred}\")\n",
    "print(f\"   GT quat:        {gt_quat}\")\n",
    "print(f\"   Predicted trans: {trans_pred}\")\n",
    "print(f\"   GT trans:        {gt_trans}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Pipeline complete! Next: train the model on LineMOD dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d830830",
   "metadata": {},
   "source": [
    "# Step 6: Training\n",
    "\n",
    "Train the complete RGB-D fusion model end-to-end:\n",
    "- **RGB Encoder**: ResNet-18 (pretrained, fine-tuned)\n",
    "- **Depth Encoder**: DepthEncoder (trained from scratch)\n",
    "- **Pose Regressor**: MLP (trained from scratch)\n",
    "\n",
    "Loss: Geodesic (rotation) + Smooth L1 (translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAIN_CONFIG = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lambda_rot': 50,    # Bilanciato con translation (loss_rot ~ 0.5 rad)\n",
    "    'lambda_trans': 0.5,   # loss_trans giÃ  scalata Ã—1000 in PoseLoss\n",
    "}\n",
    "\n",
    "print(f\"ðŸ“‹ Training configuration:\")\n",
    "for k, v in TRAIN_CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c6339",
   "metadata": {},
   "source": [
    "Integration of camera related data (K-intrinsics and bbox location)\n",
    "This ensures that no data is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84846f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_crop_meta(bbox_xywh: torch.Tensor, cam_K: torch.Tensor, img_h: int, img_w: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build scalar metadata for translation disambiguation.\n",
    "\n",
    "    bbox_xywh: (4,) [x, y, w, h] in pixels (original image)\n",
    "    cam_K: (3,3)\n",
    "    returns: (10,) float tensor\n",
    "    \"\"\"\n",
    "    x, y, w, h = bbox_xywh.float()\n",
    "    W = float(img_w)\n",
    "    H = float(img_h)\n",
    "\n",
    "    # Find BBox center in original image coordinates (normalized)\n",
    "    uc = (x + 0.5 * w) / (W + 1e-6)\n",
    "    vc = (y + 0.5 * h) / (H + 1e-6)\n",
    "\n",
    "    # BBox size (normalized)\n",
    "    wn = w / (W + 1e-6)\n",
    "    hn = h / (H + 1e-6)\n",
    "    area_n = (w * h) / ((W * H) + 1e-6)\n",
    "    ar = w / (h + 1e-6)\n",
    "\n",
    "    fx = cam_K[0, 0].float()\n",
    "    fy = cam_K[1, 1].float()\n",
    "    cx = cam_K[0, 2].float()\n",
    "    cy = cam_K[1, 2].float()\n",
    "\n",
    "    # Normalize intrinsics to image size\n",
    "    fx_n = fx / (W + 1e-6)\n",
    "    fy_n = fy / (H + 1e-6)\n",
    "    cx_n = cx / (W + 1e-6)\n",
    "    cy_n = cy / (H + 1e-6)\n",
    "\n",
    "    meta = torch.stack([uc, vc, wn, hn, area_n, ar, fx_n, fy_n, cx_n, cy_n], dim=0)\n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for RGB-D fusion training\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class LineMODFusionDataset(Dataset):\n",
    "    \"\"\"Dataset that returns RGB crop, Depth crop, and pose labels.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dataset, crop_margin=0.15, output_size=224, depth_max=2000.0, augment=False):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.crop_margin = crop_margin\n",
    "        self.output_size = output_size\n",
    "        self.depth_max = depth_max\n",
    "        self.augment = augment # Data augmentation transforms\n",
    "        self.imagenet_normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "        # Data augmentation transforms\n",
    "        if self.augment:\n",
    "            self.color_jitter = transforms.ColorJitter(\n",
    "                brightness=0.2, \n",
    "                contrast=0.2, \n",
    "                saturation=0.2,\n",
    "                hue=0.1\n",
    "            )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.base_dataset[idx]\n",
    "        \n",
    "        # Load full images\n",
    "        rgb_full = cv2.cvtColor(cv2.imread(sample['rgb_path']), cv2.COLOR_BGR2RGB)\n",
    "        depth_full = np.array(Image.open(sample['depth_path']))\n",
    "        \n",
    "        # Get bbox (convert from [x,y,w,h] to [x1,y1,x2,y2])\n",
    "        bbox = sample['bbox'].numpy()\n",
    "        x, y, w, h = bbox\n",
    "        bbox_xyxy = np.array([x, y, x+w, y+h])\n",
    "        \n",
    "        # Crop both RGB and Depth at same coordinates\n",
    "        rgb_crop = crop_roi(rgb_full, bbox_xyxy, self.crop_margin, self.output_size)\n",
    "        depth_crop = crop_roi(depth_full, bbox_xyxy, self.crop_margin, self.output_size)\n",
    "        \n",
    "        # Normalize depth to [0, 1]\n",
    "        depth_crop = np.clip(depth_crop / self.depth_max, 0, 1).astype(np.float32)\n",
    "        \n",
    "        # Convert RGB to tensor and normalize\n",
    "        rgb_tensor = torch.from_numpy(rgb_crop).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        # Data augmentation <=================\n",
    "        if self.augment:\n",
    "            # Color jitter\n",
    "            if torch.rand(1) < 0.8:\n",
    "                rgb_tensor = self.color_jitter(rgb_tensor)\n",
    "\n",
    "\n",
    "        rgb_tensor = self.imagenet_normalize(rgb_tensor)\n",
    "        \n",
    "        # Convert depth to tensor\n",
    "        depth_tensor = torch.from_numpy(depth_crop).unsqueeze(0).float()\n",
    "        \n",
    "        # Convert obj_id to tensor for proper batching\n",
    "        obj_id = sample['obj_id'] #if sample['obj_id'] is not None else -1\n",
    "        img_h, img_w = rgb_full.shape[:2]\n",
    "        meta = build_crop_meta(sample['bbox'], sample['cam_K'], img_h, img_w)\n",
    "        \n",
    "        return {\n",
    "            'rgb': rgb_tensor,\n",
    "            'depth': depth_tensor,\n",
    "            'quaternion': sample['quaternion'],\n",
    "            'translation': sample['translation'],\n",
    "            'obj_id': torch.tensor(obj_id, dtype=torch.long),\n",
    "            'meta': meta\n",
    "        }\n",
    "\n",
    "# Create dataloaders\n",
    "full_train = LineMODPoseDataset(Config.LINEMOD_ROOT, split='train')\n",
    "full_test = LineMODPoseDataset(Config.LINEMOD_ROOT, split='test')\n",
    "\n",
    "# Train/val split\n",
    "train_len = int(len(full_train) * 0.85)\n",
    "val_len = len(full_train) - train_len\n",
    "train_base, val_base = random_split(full_train, [train_len, val_len], \n",
    "                                     generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataset = LineMODFusionDataset(train_base, augment=True)   # â† Training: augment=True\n",
    "val_dataset = LineMODFusionDataset(val_base, augment=False)      # â† Validation: NO augment\n",
    "test_dataset_fusion = LineMODFusionDataset(full_test, augment=False)  # â† Test: NO augment\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_CONFIG['batch_size'], \n",
    "                          shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=TRAIN_CONFIG['batch_size'], \n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"âœ… Datasets created:\")\n",
    "print(f\"   Train: {len(train_dataset)} samples\")\n",
    "print(f\"   Val: {len(val_dataset)} samples\")\n",
    "print(f\"   Test: {len(test_dataset_fusion)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ddb2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fresh models for training\n",
    "from models.meta_encoder import MetaEncoder\n",
    "\n",
    "\n",
    "rgb_encoder_train = nn.Sequential(*list(models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1).children())[:-1])\n",
    "depth_encoder_train = DepthEncoder(output_dim=512)\n",
    "pose_regressor_train = PoseRegressor(input_dim=1056, dropout=0.3) #1024+meta10\n",
    "meta_encoder_train = MetaEncoder(input_dim=10, output_dim=32, dropout=0.1)\n",
    "\n",
    "# Move to device\n",
    "rgb_encoder_train = rgb_encoder_train.to(Config.DEVICE)\n",
    "depth_encoder_train = depth_encoder_train.to(Config.DEVICE)\n",
    "pose_regressor_train = pose_regressor_train.to(Config.DEVICE)\n",
    "meta_encoder_train = meta_encoder_train.to(Config.DEVICE)\n",
    "\n",
    "# Optimizer (all parameters)\n",
    "rgb_params = list(rgb_encoder_train.parameters())\n",
    "other_params = list(depth_encoder_train.parameters()) + \\\n",
    "               list(pose_regressor_train.parameters()) + \\\n",
    "               list(meta_encoder_train.parameters())\n",
    "\n",
    "all_params = rgb_params + other_params\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': rgb_params, 'lr': 1e-5},       # Mantieni basso\n",
    "    {'params': other_params, 'lr': 3e-4}      # â¬‡ï¸ Riduci da 1e-3\n",
    "], weight_decay=TRAIN_CONFIG['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,        # Aspetta 5 epochs\n",
    "    min_lr=1e-7,\n",
    ")\n",
    "# Loss functions\n",
    "from utils.losses import PoseLoss\n",
    "criterion = PoseLoss(lambda_trans=TRAIN_CONFIG['lambda_trans'], lambda_rot=TRAIN_CONFIG['lambda_rot'])\n",
    "\n",
    "print(f\"âœ… Models and optimizer initialized\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in all_params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ae66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_epoch(rgb_enc, depth_enc, meta_enc, pose_reg, loader, optimizer, criterion, device, log_interval=100):\n",
    "    rgb_enc.train()\n",
    "    depth_enc.train()\n",
    "    meta_enc.train()\n",
    "    pose_reg.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
    "        rgb = batch['rgb'].to(device)\n",
    "        depth = batch['depth'].to(device)\n",
    "        gt_quat = batch['quaternion'].to(device)\n",
    "        gt_trans = batch['translation'].to(device)\n",
    "        meta = batch['meta'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        f_rgb = rgb_enc(rgb).squeeze(-1).squeeze(-1)\n",
    "        f_depth = depth_enc(depth)\n",
    "        f_meta = meta_enc(meta)\n",
    "        f_fused = torch.cat([f_rgb, f_depth, f_meta], dim=1)\n",
    "        pose = pose_reg(f_fused)\n",
    "        \n",
    "        pred_quat = pose[:, :4]\n",
    "        pred_trans = pose[:, 4:]\n",
    "        \n",
    "        # Loss (PoseLoss returns a dict with 'total', 'rot', 'trans')\n",
    "        loss_dict = criterion(pred_quat, pred_trans, gt_quat, gt_trans)\n",
    "        loss = loss_dict['total']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # ðŸ“Š Logging per verificare bilanciamento loss\n",
    "        if batch_idx % log_interval == 0:\n",
    "            weighted_trans = criterion.lambda_trans * loss_dict['trans'].item()\n",
    "            weighted_rot = criterion.lambda_rot * loss_dict['rot'].item()\n",
    "            tqdm.write(f\"   [Batch {batch_idx}] loss_trans: {loss_dict['trans'].item():.4f} (Ã—{criterion.lambda_trans}={weighted_trans:.4f}) | \"\n",
    "                      f\"loss_rot: {loss_dict['rot'].item():.4f} (Ã—{criterion.lambda_rot}={weighted_rot:.4f}) | \"\n",
    "                      f\"ratio: {weighted_trans/(weighted_rot+1e-8):.2f}\")\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate_epoch(rgb_enc, depth_enc, meta_enc, pose_reg, loader, criterion, device):\n",
    "    rgb_enc.eval()\n",
    "    depth_enc.eval()\n",
    "    meta_enc.eval()\n",
    "    pose_reg.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            rgb = batch['rgb'].to(device)\n",
    "            depth = batch['depth'].to(device)\n",
    "            gt_quat = batch['quaternion'].to(device)\n",
    "            gt_trans = batch['translation'].to(device)\n",
    "            meta = batch['meta'].to(device)\n",
    "            \n",
    "            f_rgb = rgb_enc(rgb).squeeze(-1).squeeze(-1)\n",
    "            f_depth = depth_enc(depth)\n",
    "            f_meta = meta_enc(meta)\n",
    "            f_fused = torch.cat([f_rgb, f_depth, f_meta], dim=1)\n",
    "            pose = pose_reg(f_fused)\n",
    "            \n",
    "            pred_quat = pose[:, :4]\n",
    "            pred_trans = pose[:, 4:]\n",
    "            \n",
    "            loss_dict = criterion(pred_quat, pred_trans, gt_quat, gt_trans)\n",
    "            loss = loss_dict['total']\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "print(\"âœ… Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "checkpoint_dir = Config.CHECKPOINT_DIR / 'pose' / 'fusion_rgbd_512'\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸš€ Starting training for {TRAIN_CONFIG['epochs']} epochs...\")\n",
    "print(f\"   Checkpoint dir: {checkpoint_dir}\")\n",
    "\n",
    "for epoch in range(TRAIN_CONFIG['epochs']):\n",
    "    train_loss = train_epoch(\n",
    "        rgb_encoder_train, depth_encoder_train, meta_encoder_train, pose_regressor_train,\n",
    "        train_loader, optimizer, criterion, Config.DEVICE\n",
    "    )\n",
    "    val_loss = validate_epoch(\n",
    "        rgb_encoder_train, depth_encoder_train, meta_encoder_train, pose_regressor_train,\n",
    "        val_loader, criterion, Config.DEVICE\n",
    "    )\n",
    "    \n",
    "    scheduler.step(val_loss)  # â† AGGIUNGI val_loss come argomento!\n",
    "    \n",
    "    # Optional: Print current LR\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'rgb_encoder': rgb_encoder_train.state_dict(),\n",
    "            'depth_encoder': depth_encoder_train.state_dict(),\n",
    "            'meta_encoder': meta_encoder_train.state_dict(),\n",
    "            'pose_regressor': pose_regressor_train.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'val_loss': val_loss\n",
    "        }, checkpoint_dir / 'best.pt')\n",
    "        print(f\"Epoch {epoch+1:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} â­ (best) | LR: {current_lr:.6f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | LR: {current_lr:.6f}\")\n",
    "\n",
    "print(f\"\\nâœ… Training complete! Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0207c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(train_losses, label='Train Loss', color='steelblue')\n",
    "ax.plot(val_losses, label='Val Loss', color='orange')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('RGB-D Fusion Training')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸ“Š Final losses:\")\n",
    "print(f\"   Train: {train_losses[-1]:.4f}\")\n",
    "print(f\"   Val: {val_losses[-1]:.4f}\")\n",
    "print(f\"   Best Val: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cd374",
   "metadata": {},
   "source": [
    "# Step 7: Testing and Evaluation\n",
    "\n",
    "Load the best model and evaluate on test set with comprehensive metrics:\n",
    "- **ADD metric**: Average Distance of Model Points\n",
    "- **ADD-S metric**: For symmetric objects\n",
    "- **Accuracy**: Percentage of predictions within threshold\n",
    "- **Per-object performance**: Breakdown by object class\n",
    "- **Visualizations**: Qualitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "best_checkpoint_path = Config.CHECKPOINT_DIR / 'pose' / 'fusion_rgbd_512' / 'best.pt'\n",
    "\n",
    "if best_checkpoint_path.exists():\n",
    "    checkpoint = torch.load(best_checkpoint_path, map_location=Config.DEVICE)\n",
    "    \n",
    "    # Load weights into models\n",
    "    rgb_encoder_train.load_state_dict(checkpoint['rgb_encoder'])\n",
    "    depth_encoder_train.load_state_dict(checkpoint['depth_encoder'])\n",
    "    meta_encoder_train.load_state_dict(checkpoint['meta_encoder'])\n",
    "    pose_regressor_train.load_state_dict(checkpoint['pose_regressor'])\n",
    "    \n",
    "    # Ensure models are on the correct device\n",
    "    rgb_encoder_train = rgb_encoder_train.to(Config.DEVICE)\n",
    "    depth_encoder_train = depth_encoder_train.to(Config.DEVICE)\n",
    "    meta_encoder_train = meta_encoder_train.to(Config.DEVICE)\n",
    "    pose_regressor_train = pose_regressor_train.to(Config.DEVICE)\n",
    "    \n",
    "    print(f\"âœ… Best checkpoint loaded from: {best_checkpoint_path}\")\n",
    "    print(f\"   Device: {Config.DEVICE}\")\n",
    "    print(f\"   Epoch: {checkpoint['epoch'] + 1}\")\n",
    "    print(f\"   Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    # Set to eval mode\n",
    "    rgb_encoder_train.eval()\n",
    "    depth_encoder_train.eval()\n",
    "    meta_encoder_train.eval()\n",
    "    pose_regressor_train.eval()\n",
    "else:\n",
    "    print(f\"âš ï¸ No checkpoint found at {best_checkpoint_path}\")\n",
    "    print(f\"   Using current model weights (may not be optimal)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3D models and info for ADD metric computation\n",
    "from utils.metrics import load_all_models, load_models_info, compute_add_batch_full_pose\n",
    "\n",
    "# Load models info (diameters, etc.)\n",
    "models_info = load_models_info(Config.MODELS_PATH / 'models_info.yml')\n",
    "\n",
    "# Load 3D models (for ADD computation)\n",
    "models_dict = load_all_models(Config.MODELS_PATH)\n",
    "\n",
    "print(f\"\\nâœ… Ready for ADD metric computation\")\n",
    "print(f\"   Objects loaded: {len(models_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test set\n",
    "from utils.transforms import quaternion_to_rotation_matrix_batch\n",
    "\n",
    "# Create test dataloader\n",
    "test_loader = DataLoader(test_dataset_fusion, batch_size=TRAIN_CONFIG['batch_size'], \n",
    "                         shuffle=False, num_workers=0)\n",
    "\n",
    "# Storage for predictions and ground truth\n",
    "all_pred_R = []\n",
    "all_pred_t = []\n",
    "all_gt_R = []\n",
    "all_gt_t = []\n",
    "all_obj_ids = []\n",
    "print(f\"ðŸ” Running inference on test set ({len(test_dataset_fusion)} samples)...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        rgb = batch['rgb'].to(Config.DEVICE)\n",
    "        depth = batch['depth'].to(Config.DEVICE)\n",
    "        gt_quat = batch['quaternion'].to(Config.DEVICE)\n",
    "        gt_trans = batch['translation'].to(Config.DEVICE)\n",
    "        meta = batch['meta'].to(Config.DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        f_rgb = rgb_encoder_train(rgb).squeeze(-1).squeeze(-1)\n",
    "        f_depth = depth_encoder_train(depth)\n",
    "        f_meta = meta_encoder_train(meta)\n",
    "        f_fused = torch.cat([f_rgb, f_depth, f_meta], dim=1)\n",
    "        pose = pose_regressor_train(f_fused)\n",
    "        \n",
    "        pred_quat = pose[:, :4]\n",
    "        pred_trans = pose[:, 4:]\n",
    "        \n",
    "        # Convert quaternions to rotation matrices (keep on device)\n",
    "        pred_R = quaternion_to_rotation_matrix_batch(pred_quat.cpu()).to(Config.DEVICE)\n",
    "        gt_R = quaternion_to_rotation_matrix_batch(gt_quat.cpu()).to(Config.DEVICE)\n",
    "        \n",
    "        # Store results (keep on device)\n",
    "        all_pred_R.append(pred_R)\n",
    "        all_pred_t.append(pred_trans)\n",
    "        all_gt_R.append(gt_R)\n",
    "        all_gt_t.append(gt_trans)\n",
    "        \n",
    "        # Get object IDs from batch\n",
    "        obj_ids_batch = batch['obj_id'].to(Config.DEVICE)\n",
    "        if isinstance(obj_ids_batch, torch.Tensor):\n",
    "            obj_ids_batch = obj_ids_batch.tolist()\n",
    "        all_obj_ids.extend(obj_ids_batch)\n",
    "\n",
    "# Concatenate all results - keep as PyTorch tensors\n",
    "# compute_add_batch_full_pose will use GPU version if Config.GPU_PRESENT=True\n",
    "all_pred_R = torch.cat(all_pred_R, dim=0)  # (N, 3, 3) - stays on device\n",
    "all_pred_t = torch.cat(all_pred_t, dim=0)  # (N, 3) - stays on device\n",
    "all_gt_R = torch.cat(all_gt_R, dim=0)      # (N, 3, 3) - stays on device\n",
    "all_gt_t = torch.cat(all_gt_t, dim=0)      # (N, 3) - stays on device\n",
    "\n",
    "print(f\"âœ… Inference complete!\")\n",
    "print(f\"   Total predictions: {len(all_pred_R)}\")\n",
    "print(f\"   Unique objects: {len(set(all_obj_ids))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7be59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ADD metrics\n",
    "print(\"ðŸ“Š Computing ADD metrics...\")\n",
    "#all_pred_t = all_pred_t / 1000  \n",
    "#all_gt_t = all_gt_t / 1000\n",
    "\n",
    "results = compute_add_batch_full_pose(\n",
    "    pred_R_batch=all_pred_R,\n",
    "    pred_t_batch=all_pred_t,\n",
    "    gt_R_batch=all_gt_R,\n",
    "    gt_t_batch=all_gt_t,\n",
    "    obj_ids=all_obj_ids,\n",
    "    models_dict=models_dict,\n",
    "    models_info=models_info,\n",
    "    symmetric_objects=Config.SYMMETRIC_OBJECTS,\n",
    "    threshold=Config.ADD_THRESHOLD\n",
    ")\n",
    "\n",
    "# Print overall metrics\n",
    "print(f\"\\nâœ… Overall Test Results:\")\n",
    "print(f\"   Mean ADD: {results['mean_add']:.2f} mm\")\n",
    "print(f\"   Accuracy: {results['accuracy']*100:.2f}% (< {Config.ADD_THRESHOLD*100:.0f}% diameter)\")\n",
    "print(f\"   Total samples: {len(results['add_values'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed22b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-object analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Group results by object\n",
    "per_object_results = {}\n",
    "\n",
    "for obj_id in set(all_obj_ids):\n",
    "    # Get indices for this object\n",
    "    indices = [i for i, oid in enumerate(all_obj_ids) if oid == obj_id]\n",
    "    \n",
    "    # Extract metrics for this object\n",
    "    obj_add_values = [results['add_values'][i] for i in indices]\n",
    "    obj_is_correct = [results['is_correct'][i] for i in indices]\n",
    "    \n",
    "    # Compute statistics\n",
    "    per_object_results[obj_id] = {\n",
    "        'count': len(indices),\n",
    "        'mean_add': np.mean(obj_add_values),\n",
    "        'median_add': np.median(obj_add_values),\n",
    "        'std_add': np.std(obj_add_values),\n",
    "        'accuracy': np.mean(obj_is_correct) * 100,\n",
    "        'diameter': models_info[obj_id]['diameter'],\n",
    "        'symmetric': obj_id in Config.SYMMETRIC_OBJECTS\n",
    "    }\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df_results = pd.DataFrame.from_dict(per_object_results, orient='index')\n",
    "df_results.index.name = 'Object ID'\n",
    "df_results = df_results.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Per-Object Performance:\")\n",
    "print(df_results.to_string())\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nðŸ“ˆ Summary Statistics:\")\n",
    "print(f\"   Best accuracy: {df_results['accuracy'].max():.2f}% (Object {df_results['accuracy'].idxmax()})\")\n",
    "print(f\"   Worst accuracy: {df_results['accuracy'].min():.2f}% (Object {df_results['accuracy'].idxmin()})\")\n",
    "print(f\"   Mean accuracy: {df_results['accuracy'].mean():.2f}%\")\n",
    "print(f\"   Median accuracy: {df_results['accuracy'].median():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a08706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-object results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Accuracy by object\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['green' if acc >= 70 else 'orange' if acc >= 50 else 'red' \n",
    "          for acc in df_results['accuracy']]\n",
    "ax1.barh(range(len(df_results)), df_results['accuracy'], color=colors)\n",
    "ax1.set_yticks(range(len(df_results)))\n",
    "ax1.set_yticklabels([f\"Obj {idx}\" for idx in df_results.index])\n",
    "ax1.set_xlabel('Accuracy (%)')\n",
    "ax1.set_title('Per-Object Accuracy (< 10% diameter)')\n",
    "ax1.axvline(x=50, color='gray', linestyle='--', alpha=0.5, label='50%')\n",
    "ax1.axvline(x=70, color='gray', linestyle='--', alpha=0.5, label='70%')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Mean ADD by object\n",
    "ax2 = axes[0, 1]\n",
    "ax2.barh(range(len(df_results)), df_results['mean_add'], color='steelblue')\n",
    "ax2.set_yticks(range(len(df_results)))\n",
    "ax2.set_yticklabels([f\"Obj {idx}\" for idx in df_results.index])\n",
    "ax2.set_xlabel('Mean ADD (mm)')\n",
    "ax2.set_title('Per-Object Mean ADD')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. ADD distribution (histogram)\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(results['add_values'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(x=results['mean_add'], color='red', linestyle='--', linewidth=2, label=f'Mean: {results[\"mean_add\"]:.1f} mm')\n",
    "ax3.axvline(x=np.median(results['add_values']), color='orange', linestyle='--', linewidth=2, \n",
    "            label=f'Median: {np.median(results[\"add_values\"]):.1f} mm')\n",
    "ax3.set_xlabel('ADD (mm)')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('ADD Distribution Across All Samples')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Accuracy vs Object Diameter\n",
    "ax4 = axes[1, 1]\n",
    "symmetric_mask = df_results['symmetric']\n",
    "ax4.scatter(df_results[~symmetric_mask]['diameter'], \n",
    "           df_results[~symmetric_mask]['accuracy'],\n",
    "           c='steelblue', s=100, alpha=0.7, label='Non-symmetric')\n",
    "ax4.scatter(df_results[symmetric_mask]['diameter'], \n",
    "           df_results[symmetric_mask]['accuracy'],\n",
    "           c='orange', s=100, alpha=0.7, label='Symmetric')\n",
    "\n",
    "# Annotate points\n",
    "for idx, row in df_results.iterrows():\n",
    "    ax4.annotate(f'{idx}', (row['diameter'], row['accuracy']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "ax4.set_xlabel('Object Diameter (mm)')\n",
    "ax4.set_ylabel('Accuracy (%)')\n",
    "ax4.set_title('Accuracy vs Object Size')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaae2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute additional metrics for paper\n",
    "from scipy.spatial.transform import Rotation as R_scipy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Rotation errors (in degrees)\n",
    "rotation_errors = []\n",
    "\n",
    "for i in range(len(all_pred_R)):\n",
    "    # Compute relative rotation: R_error = R_pred^T @ R_gt\n",
    "    R_error = all_pred_R[i].T @ all_gt_R[i]\n",
    "    \n",
    "    # Convert to CPU and numpy before using scipy\n",
    "    R_error_np = R_error.cpu().numpy()\n",
    "    \n",
    "    # Convert to angle-axis and extract angle\n",
    "    r = R_scipy.from_matrix(R_error_np)\n",
    "    angle = np.linalg.norm(r.as_rotvec())  # Rotation angle in radians\n",
    "    angle_deg = np.degrees(angle)\n",
    "    \n",
    "    rotation_errors.append(angle_deg)\n",
    "    # Translation errors (in mm)\n",
    "    translation_errors = torch.norm(all_pred_t - all_gt_t, dim=1).cpu().numpy() * 1000  # Convert to mm\n",
    "    R_error = all_pred_R[i].T @ all_gt_R[i]\n",
    "    \n",
    "    # Convert to CPU and numpy before using scipy\n",
    "    R_error_np = R_error.cpu().numpy()\n",
    "    \n",
    "    # Convert to angle-axis and extract angle\n",
    "    r = R_scipy.from_matrix(R_error_np)\n",
    "    angle = np.linalg.norm(r.as_rotvec())  # Rotation angle in radians\n",
    "    angle_deg = np.degrees(angle)\n",
    "    \n",
    "    rotation_errors.append(angle_deg)\n",
    "\n",
    "rotation_errors = np.array(rotation_errors)\n",
    "\n",
    "print(f\"\\nðŸ”„ Rotation Errors:\")\n",
    "print(f\"   Mean: {rotation_errors.mean():.2f}Â°\")\n",
    "print(f\"   Median: {np.median(rotation_errors):.2f}Â°\")\n",
    "print(f\"   Std: {rotation_errors.std():.2f}Â°\")\n",
    "print(f\"   Max: {rotation_errors.max():.2f}Â°\")\n",
    "\n",
    "print(f\"\\nðŸ“ Translation Errors:\")\n",
    "# Translation errors (in mm)\n",
    "translation_errors = F.smooth_l1_loss(all_pred_t*1000, all_gt_t*1000).cpu().numpy()  # Convert to mm\n",
    "print(f\"   Std: {translation_errors.std():.2f} mm\")\n",
    "print(f\"   Max: {translation_errors.max():.2f} mm\")\n",
    "\n",
    "# Accuracy at different thresholds (for paper)\n",
    "thresholds = [0.02, 0.05, 0.10, 0.15, 0.20]  # Fractions of diameter\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Accuracy at Different Thresholds:\")\n",
    "for thresh in thresholds:\n",
    "    # Recompute ADD metric with this threshold\n",
    "    results_thresh = compute_add_batch_full_pose(\n",
    "        pred_R_batch=all_pred_R,\n",
    "        pred_t_batch=all_pred_t,\n",
    "        gt_R_batch=all_gt_R,\n",
    "        gt_t_batch=all_gt_t,\n",
    "        obj_ids=all_obj_ids,\n",
    "        models_dict=models_dict,\n",
    "        models_info=models_info,\n",
    "        symmetric_objects=Config.SYMMETRIC_OBJECTS,\n",
    "        threshold=thresh\n",
    "    )\n",
    "    print(f\"   {thresh*100:2.0f}% diameter: {results_thresh['accuracy']*100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f64e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-sample translation errors (in mm)\n",
    "translation_errors = torch.norm(all_pred_t - all_gt_t, dim=1).cpu().numpy() * 1000\n",
    "\n",
    "# Visualize rotation and translation errors\n",
    "# Ensure rotation_errors and translation_errors have the same length\n",
    "rotation_errors_plot = rotation_errors[:len(translation_errors)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Rotation error distribution\n",
    "ax1 = axes[0]\n",
    "ax1.hist(rotation_errors_plot, bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(x=rotation_errors_plot.mean(), color='red', linestyle='--', linewidth=2,\n",
    "           label=f'Mean: {rotation_errors_plot.mean():.1f}Â°')\n",
    "ax1.axvline(x=np.median(rotation_errors_plot), color='orange', linestyle='--', linewidth=2,\n",
    "           label=f'Median: {np.median(rotation_errors_plot):.1f}Â°')\n",
    "ax1.set_xlabel('Rotation Error (degrees)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Rotation Error Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Translation error distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(translation_errors, bins=50, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=translation_errors.mean(), color='red', linestyle='--', linewidth=2,\n",
    "           label=f'Mean: {translation_errors.mean():.1f} mm')\n",
    "ax2.axvline(x=np.median(translation_errors), color='orange', linestyle='--', linewidth=2,\n",
    "           label=f'Median: {np.median(translation_errors):.1f} mm')\n",
    "ax2.set_xlabel('Translation Error (mm)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Translation Error Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Rotation vs Translation error scatter\n",
    "ax3 = axes[2]\n",
    "scatter = ax3.scatter(rotation_errors_plot, translation_errors, \n",
    "                     c=results['add_values'], cmap='viridis', \n",
    "                     alpha=0.6, s=20)\n",
    "ax3.set_xlabel('Rotation Error (degrees)')\n",
    "ax3.set_ylabel('Translation Error (mm)')\n",
    "ax3.set_title('Rotation vs Translation Error')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax3)\n",
    "cbar.set_label('ADD (mm)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Error analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative visualization: show best and worst predictions\n",
    "# Find best and worst samples\n",
    "add_array = np.array(results['add_values'])\n",
    "best_indices = np.argsort(add_array)[:6]  # Top 6 best\n",
    "worst_indices = np.argsort(add_array)[-6:]  # Top 6 worst\n",
    "\n",
    "def visualize_prediction(idx, title_prefix=\"\"):\n",
    "    \"\"\"Visualize a single prediction.\"\"\"\n",
    "    # Get the sample from test dataset\n",
    "    sample = test_dataset_fusion[idx]\n",
    "    \n",
    "    # Get predictions\n",
    "    pred_R = all_pred_R[idx]\n",
    "    pred_t = all_pred_t[idx]\n",
    "    gt_R = all_gt_R[idx]\n",
    "    gt_t = all_gt_t[idx]\n",
    "    \n",
    "    # Get object info\n",
    "    obj_id = all_obj_ids[idx]\n",
    "    add_value = results['add_values'][idx]\n",
    "    is_correct = results['is_correct'][idx]\n",
    "    \n",
    "    # Display info\n",
    "    status = \"âœ… Correct\" if is_correct else \"âŒ Incorrect\"\n",
    "    \n",
    "    return {\n",
    "        'rgb': sample['rgb'].permute(1, 2, 0).numpy(),\n",
    "        'depth': sample['depth'].squeeze().numpy(),\n",
    "        'obj_id': obj_id,\n",
    "        'add': add_value,\n",
    "        'status': status,\n",
    "        'rot_error': rotation_errors[idx],\n",
    "        'trans_error': translation_errors[idx]\n",
    "    }\n",
    "\n",
    "# Visualize best predictions\n",
    "fig, axes = plt.subplots(2, 6, figsize=(20, 7))\n",
    "fig.suptitle('Best Predictions (Lowest ADD)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, idx in enumerate(best_indices):\n",
    "    info = visualize_prediction(idx, \"Best\")\n",
    "    \n",
    "    # RGB\n",
    "    ax_rgb = axes[0, i]\n",
    "    ax_rgb.imshow((info['rgb'] * 0.229 + 0.456).clip(0, 1))  # Denormalize (approximate)\n",
    "    ax_rgb.set_title(f\"Obj {info['obj_id']}\\n{info['status']}\", fontsize=9)\n",
    "    ax_rgb.axis('off')\n",
    "    \n",
    "    # Depth\n",
    "    ax_depth = axes[1, i]\n",
    "    ax_depth.imshow(info['depth'], cmap='viridis')\n",
    "    ax_depth.set_title(f\"ADD: {info['add']:.1f}mm\\nRot: {info['rot_error']:.1f}Â°\", fontsize=9)\n",
    "    ax_depth.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize worst predictions\n",
    "fig, axes = plt.subplots(2, 6, figsize=(20, 7))\n",
    "fig.suptitle('Worst Predictions (Highest ADD)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, idx in enumerate(worst_indices):\n",
    "    info = visualize_prediction(idx, \"Worst\")\n",
    "    \n",
    "    # RGB\n",
    "    ax_rgb = axes[0, i]\n",
    "    ax_rgb.imshow((info['rgb'] * 0.229 + 0.456).clip(0, 1))  # Denormalize (approximate)\n",
    "    ax_rgb.set_title(f\"Obj {info['obj_id']}\\n{info['status']}\", fontsize=9)\n",
    "    ax_rgb.axis('off')\n",
    "    \n",
    "    # Depth\n",
    "    ax_depth = axes[1, i]\n",
    "    ax_depth.imshow(info['depth'], cmap='viridis')\n",
    "    ax_depth.set_title(f\"ADD: {info['add']:.1f}mm\\nRot: {info['rot_error']:.1f}Â°\", fontsize=9)\n",
    "    ax_depth.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Qualitative results visualized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933331db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table for paper\n",
    "summary_table = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Overall Accuracy (10% diameter)',\n",
    "        'Mean ADD (mm)',\n",
    "        'Median ADD (mm)',\n",
    "        'Mean Rotation Error (Â°)',\n",
    "        'Median Rotation Error (Â°)',\n",
    "        'Mean Translation Error (mm)',\n",
    "        'Median Translation Error (mm)',\n",
    "        'Test Samples',\n",
    "        'Objects',\n",
    "        'Model Parameters'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{results['accuracy']*100:.2f}%\",\n",
    "        f\"{results['mean_add']:.2f}\",\n",
    "        f\"{np.median(results['add_values']):.2f}\",\n",
    "        f\"{rotation_errors.mean():.2f}\",\n",
    "        f\"{np.median(rotation_errors):.2f}\",\n",
    "        f\"{translation_errors.mean():.2f}\",\n",
    "        f\"{np.median(translation_errors):.2f}\",\n",
    "        f\"{len(test_dataset_fusion)}\",\n",
    "        f\"{len(set(all_obj_ids))}\",\n",
    "        f\"{sum(p.numel() for p in all_params):,}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“‹ FINAL RESULTS SUMMARY - RGB-D FUSION MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(summary_table.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export results to CSV for paper\n",
    "results_dir = Config.CHECKPOINT_DIR / 'pose' / 'fusion_rgbd_512'\n",
    "df_results.to_csv(results_dir / 'per_object_results.csv')\n",
    "summary_table.to_csv(results_dir / 'summary_results.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… Results saved to: {results_dir}\")\n",
    "print(f\"   - per_object_results.csv\")\n",
    "print(f\"   - summary_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification of measurement units and scales\n",
    "print(\"ðŸ” VERIFICATION OF MEASUREMENT UNITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Check translation magnitudes from dataset\n",
    "sample_translations = []\n",
    "for i in range(min(100, len(test_dataset_fusion))):\n",
    "    sample = test_dataset_fusion[i]\n",
    "    trans = sample['translation'].numpy()\n",
    "    sample_translations.append(trans)\n",
    "\n",
    "sample_translations = np.array(sample_translations)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Ground Truth Translation Analysis:\")\n",
    "print(f\"   Mean magnitude: {np.linalg.norm(sample_translations, axis=1).mean():.4f}\")\n",
    "print(f\"   Min magnitude: {np.linalg.norm(sample_translations, axis=1).min():.4f}\")\n",
    "print(f\"   Max magnitude: {np.linalg.norm(sample_translations, axis=1).max():.4f}\")\n",
    "print(f\"   Mean X: {sample_translations[:, 0].mean():.4f}\")\n",
    "print(f\"   Mean Y: {sample_translations[:, 1].mean():.4f}\")\n",
    "print(f\"   Mean Z: {sample_translations[:, 2].mean():.4f}\")\n",
    "\n",
    "# Determine if values are in meters or millimeters\n",
    "if np.linalg.norm(sample_translations, axis=1).mean() < 10:\n",
    "    print(f\"   âœ… Units appear to be in METERS (typical range 0.3-2.0m)\")\n",
    "else:\n",
    "    print(f\"   âœ… Units appear to be in MILLIMETERS (typical range 300-2000mm)\")\n",
    "\n",
    "# 2. Check prediction magnitudes\n",
    "print(\"\\n2ï¸âƒ£ Predicted Translation Analysis:\")\n",
    "pred_magnitudes = torch.norm(all_pred_t, dim=1).cpu().numpy()\n",
    "print(f\"   Mean magnitude: {pred_magnitudes.mean():.4f}\")\n",
    "print(f\"   Min magnitude: {pred_magnitudes.min():.4f}\")\n",
    "print(f\"   Max magnitude: {pred_magnitudes.max():.4f}\")\n",
    "print(f\"   Mean X: {all_pred_t[:, 0].cpu().numpy().mean():.4f}\")\n",
    "print(f\"   Mean Y: {all_pred_t[:, 1].cpu().numpy().mean():.4f}\")\n",
    "print(f\"   Mean Z: {all_pred_t[:, 2].cpu().numpy().mean():.4f}\")\n",
    "\n",
    "if pred_magnitudes.mean() < 10:\n",
    "    print(f\"   âœ… Predictions are in METERS\")\n",
    "else:\n",
    "    print(f\"   âœ… Predictions are in MILLIMETERS\")\n",
    "\n",
    "# 3. Check object diameters (should be in mm)\n",
    "print(\"\\n3ï¸âƒ£ Object Diameter Analysis (from models_info):\")\n",
    "diameters = [info['diameter'] for info in models_info.values()]\n",
    "print(f\"   Mean diameter: {np.mean(diameters):.2f} mm\")\n",
    "print(f\"   Min diameter: {np.min(diameters):.2f} mm\")\n",
    "print(f\"   Max diameter: {np.max(diameters):.2f} mm\")\n",
    "print(f\"   âœ… Diameters are in MILLIMETERS (expected range ~70-300mm)\")\n",
    "\n",
    "# 4. Verify ADD computation units\n",
    "print(\"\\n4ï¸âƒ£ ADD Metric Units:\")\n",
    "print(f\"   Mean ADD: {results['mean_add']:.2f} mm\")\n",
    "print(f\"   Threshold used: {Config.ADD_THRESHOLD*100:.0f}% of diameter\")\n",
    "sample_diameter = models_info[all_obj_ids[0]]['diameter']\n",
    "print(f\"   Example: Object {all_obj_ids[0]} diameter = {sample_diameter:.2f} mm\")\n",
    "print(f\"   Threshold = {Config.ADD_THRESHOLD * sample_diameter:.2f} mm\")\n",
    "\n",
    "# 5. Check consistency\n",
    "print(\"\\n5ï¸âƒ£ Consistency Check:\")\n",
    "gt_mean_mag = np.linalg.norm(sample_translations, axis=1).mean()\n",
    "pred_mean_mag = pred_magnitudes.mean()\n",
    "ratio = pred_mean_mag / gt_mean_mag\n",
    "\n",
    "print(f\"   GT mean magnitude: {gt_mean_mag:.4f}\")\n",
    "print(f\"   Pred mean magnitude: {pred_mean_mag:.4f}\")\n",
    "print(f\"   Ratio (pred/gt): {ratio:.4f}\")\n",
    "\n",
    "if 0.5 < ratio < 2.0:\n",
    "    print(f\"   âœ… Predictions and GT are in SAME SCALE\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ WARNING: Predictions and GT might be in DIFFERENT SCALES\")\n",
    "    if ratio > 100:\n",
    "        print(f\"   âš ï¸ Predictions might be in mm while GT is in meters!\")\n",
    "    elif ratio < 0.01:\n",
    "        print(f\"   âš ï¸ GT might be in mm while predictions are in meters!\")\n",
    "\n",
    "# 6. Translation error sanity check\n",
    "print(\"\\n6ï¸âƒ£ Translation Error Sanity Check:\")\n",
    "print(f\"   Mean translation error: {translation_errors.mean():.2f} mm\")\n",
    "print(f\"   Median translation error: {np.median(translation_errors):.2f} mm\")\n",
    "\n",
    "# Typical LineMOD translation errors should be 10-50mm for good models\n",
    "if 5 < translation_errors.mean() < 100:\n",
    "    print(f\"   âœ… Translation errors are in REASONABLE RANGE for LineMOD\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ Translation errors seem unusual - check unit conversion!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7d429",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Experiment Complete!\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **RGB-D Fusion Model** successfully combines RGB and Depth features for 6D pose estimation\n",
    "2. **End-to-end training** with ResNet-18 (RGB) + DepthEncoder + MLP regressor\n",
    "3. **Comprehensive evaluation** with ADD metrics, per-object analysis, and error distributions\n",
    "\n",
    "### Architecture:\n",
    "- RGB Branch: ResNet-18 â†’ 512-dim features\n",
    "- Depth Branch: DepthEncoder â†’ 512-dim features  \n",
    "- Fusion: Concatenation â†’ 1024-dim (balanced 50/50 contribution)\n",
    "- Regressor: MLP â†’ 7-dim pose (quaternion + translation)\n",
    "\n",
    "### For the Paper:\n",
    "\n",
    "- âœ… Overall accuracy and ADD metrics computed\n",
    "- âœ… Per-object performance breakdown\n",
    "- âœ… Rotation and translation error analysis  \n",
    "- âœ… Accuracy at multiple thresholds (2%, 5%, 10%, 15%, 20% diameter)\n",
    "- âœ… Qualitative results (best/worst predictions)\n",
    "- âœ… Results exported to CSV for tables\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Compare with baseline model (rotation-only)\n",
    "- Ablation studies (RGB-only vs Depth-only vs Fusion)\n",
    "- Hyperparameter tuning (learning rate, weight decay, lambda values)\n",
    "- Extended training (more epochs, data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity = compute_add_batch_full_pose(\n",
    "    pred_R_batch=all_gt_R,\n",
    "    pred_t_batch=all_gt_t,\n",
    "    gt_R_batch=all_gt_R,\n",
    "    gt_t_batch=all_gt_t,\n",
    "    obj_ids=all_obj_ids,\n",
    "    models_dict=models_dict,\n",
    "    models_info=models_info,\n",
    "    symmetric_objects=Config.SYMMETRIC_OBJECTS,\n",
    "    threshold=Config.ADD_THRESHOLD\n",
    ")\n",
    "print(sanity[\"mean_add\"], sanity[\"accuracy\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
